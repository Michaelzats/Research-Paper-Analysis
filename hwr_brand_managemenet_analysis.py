# -*- coding: utf-8 -*-
"""HWR Brand Managemenet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gkOsaLqynsWsEjNhqBWAzU06Joil6PJy

# **ALL RESULTS**

The following report goes as the attachment for the study of The Impact of Customers' Cultural Differences from East and West Germany on Success Factors of co-branding collaborations. The used datasets in CSV format and EXCEL sheet (data cleaning) can be found in the Github account
"""

import pandas as pd
from scipy import stats

df_3 = pd.read_csv('HWR Brand Management - Copy of Copy of RAW DATA 1 without other country.csv')


print(df_3.describe())  # get descriptive statistics
# df_1['Co-branding That brands könnte ein hochwertiges Partygetränk hervorbringen'].hist()

df_east = df_3[df_3['Aus welchem Teil Deutschlands kommst du?'] == 1]

print(df_east.describe(include='all'))

non_numerical_columns = [
    "East + East (Quality product from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    ""
    "East + East (Quality product from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "West + West (Quality product from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "East + West (Quality product from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "US + West (Quality product from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "US + East (Quality product from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "US + US (Quality product from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "East + East (Ready to Recommend from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "West + West (Ready to Recommend from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "East + West (Ready to Recommend from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "US + West (Ready to Recommend from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "US + East (Ready to Recommend from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)",
    "US + US (Ready to Recommend from the cooperation from the brands of that countiries, 1 (bad) - 7 (good)"
]
print(df_east[non_numerical_columns].describe())

import pandas as pd
from scipy import stats

# List of columns in the DataFrame
columns = df_3.columns.tolist()

# Remove the column "Aus welchem Teil Deutschlands kommst du?" from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Calculate correlation and p-value of the column "Aus welchem Teil Deutschlands kommst du?" with all other columns
for col in columns:
    # Exclude NA/null values
    df_clean = df_3[['Aus welchem Teil Deutschlands kommst du?', col]].dropna()

    # Check if there are at least 2 non-null values
    if len(df_clean) >= 2:
        correlation, p_value = stats.pearsonr(df_clean['Aus welchem Teil Deutschlands kommst du?'], df_clean[col])
        print(f'Correlation of region with {col}: {correlation}, p-value: {p_value}')
    else:
        print(f'Not enough data to compute correlation between region and {col}')

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# # Save old column names
# old_column_names = df_3.columns.tolist()

# # Renaming the columns to v_1, v_2, v_3, etc.
# df_v = df_3.columns = ['v_' + str(i+1) for i in range(df_3.shape[1])]

# # Print out a mapping of old column names to new column names
# new_column_names = df_3.columns.tolist()
# column_name_mapping = dict(zip(new_column_names, old_column_names))
# for new_name, old_name in column_name_mapping.items():
#     print(f"{new_name} represents {old_name}")

correlation_matrix = df_3.corr()

# Create a larger figure before generating the heatmap
plt.figure(figsize=(40, 40))

sns.heatmap(data = correlation_matrix, annot = True)
plt.show()

from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy import stats
import numpy as np

columns = df_3.columns.tolist()

# Remove 'Aus welchem Teil Deutschlands kommst du?' from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Perform ANOVA and Tukey's HSD test for each column
for col in columns:
    group1 = df_3[df_3['Aus welchem Teil Deutschlands kommst du?'] == 1][col]
    group2 = df_3[df_3['Aus welchem Teil Deutschlands kommst du?'] == 2][col]

    # Concatenate the groups and create a list with labels
    values = pd.concat([group1, group2])
    labels = ['group1'] * len(group1) + ['group2'] * len(group2)

    # Perform the ANOVA
    f_val, p_val = stats.f_oneway(group1, group2)

    # Check for NaNs in ANOVA results
    if not (np.isnan(f_val) or np.isnan(p_val)):
        print(f'ANOVA results for {col}:\nF-value: {f_val}\np-value: {p_val}\n')

    # Perform Tukey's HSD test
    tukey = pairwise_tukeyhsd(endog=values,     # Data
                              groups=labels,   # Groups
                              alpha=0.05)      # Significance level

    # Print the result
    if not np.isnan(tukey.meandiffs).any():  # Check for NaNs in Tukey HSD results
        print(f"Tukey's HSD test results for {col}:\n{tukey}\n")

"""***EAST + EAST***"""

df_4 = pd.read_csv('HWR Brand Management - EAST+EAST (3).csv')
print(df_4.describe())  # get descriptive statistics

import pandas as pd
from scipy import stats

# List of columns in the DataFrame
columns = df_4.columns.tolist()

# Remove the column "Aus welchem Teil Deutschlands kommst du?" from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Calculate correlation and p-value of the column "Aus welchem Teil Deutschlands kommst du?" with all other columns
for col in columns:
    # Exclude NA/null values
    df_clean = df_4[['Aus welchem Teil Deutschlands kommst du?', col]].dropna()

    # Check if there are at least 2 non-null values
    if len(df_clean) >= 2:
        correlation, p_value = stats.pearsonr(df_clean['Aus welchem Teil Deutschlands kommst du?'], df_clean[col])
        print(f'Correlation of region with {col}: {correlation}, p-value: {p_value}')
    else:
        print(f'Not enough data to compute correlation between region and {col}')

from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy import stats
import numpy as np

columns = df_4.columns.tolist()

# Remove 'Aus welchem Teil Deutschlands kommst du?' from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Perform ANOVA and Tukey's HSD test for each column
for col in columns:
    group1 = df_4[df_4['Aus welchem Teil Deutschlands kommst du?'] == 1][col]
    group2 = df_4[df_4['Aus welchem Teil Deutschlands kommst du?'] == 2][col]

    # Concatenate the groups and create a list with labels
    values = pd.concat([group1, group2])
    labels = ['group1'] * len(group1) + ['group2'] * len(group2)

    # Perform the ANOVA
    f_val, p_val = stats.f_oneway(group1, group2)

    # Check for NaNs in ANOVA results
    if not (np.isnan(f_val) or np.isnan(p_val)):
        print(f'ANOVA results for {col}:\nF-value: {f_val}\np-value: {p_val}\n')

    # Perform Tukey's HSD test
    tukey = pairwise_tukeyhsd(endog=values,     # Data
                              groups=labels,   # Groups
                              alpha=0.05)      # Significance level

    # Print the result
    if not np.isnan(tukey.meandiffs).any():  # Check for NaNs in Tukey HSD results        print(f"Tukey's HSD test results for {col}:\n{tukey}\n")

"""***EAST ALL BRANDS ISNIDE OF VARIATIONS WHERE EAST WAS***"""

df_5 = pd.read_csv('HWR Brand Management - WITH EAST BRANDS ALL.csv')
print(df_5.describe())  # get descriptive statistics

import pandas as pd
from scipy import stats

# List of columns in the DataFrame
columns = df_5.columns.tolist()

# Remove the column "Aus welchem Teil Deutschlands kommst du?" from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Calculate correlation and p-value of the column "Aus welchem Teil Deutschlands kommst du?" with all other columns
for col in columns:
    # Exclude NA/null values
    df_clean = df_5[['Aus welchem Teil Deutschlands kommst du?', col]].dropna()

    # Check if there are at least 2 non-null values
    if len(df_clean) >= 2:
        correlation, p_value = stats.pearsonr(df_clean['Aus welchem Teil Deutschlands kommst du?'], df_clean[col])
        print(f'Correlation of region with {col}: {correlation}, p-value: {p_value}')
    else:
        print(f'Not enough data to compute correlation between region and {col}')

from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy import stats
import numpy as np

columns = df_5.columns.tolist()

# Remove 'Aus welchem Teil Deutschlands kommst du?' from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Perform ANOVA and Tukey's HSD test for each column
for col in columns:
    group1 = df_5[df_5['Aus welchem Teil Deutschlands kommst du?'] == 1][col]
    group2 = df_5[df_5['Aus welchem Teil Deutschlands kommst du?'] == 2][col]

    # Concatenate the groups and create a list with labels
    values = pd.concat([group1, group2])
    labels = ['group1'] * len(group1) + ['group2'] * len(group2)

    # Perform the ANOVA
    f_val, p_val = stats.f_oneway(group1, group2)

    # Check for NaNs in ANOVA results
    if not (np.isnan(f_val) or np.isnan(p_val)):
        print(f'ANOVA results for {col}:\nF-value: {f_val}\np-value: {p_val}\n')

    # Perform Tukey's HSD test
    tukey = pairwise_tukeyhsd(endog=values,     # Data
                              groups=labels,   # Groups
                              alpha=0.05)      # Significance level

    # Print the result
    if not np.isnan(tukey.meandiffs).any():  # Check for NaNs in Tukey HSD results
        print(f"Tukey's HSD test results for {col}:\n{tukey}\n")

"""***WEST ALL BRANDS ISNIDE OF VARIATIONS WHERE WEST WAS***"""

df_6 = pd.read_csv('HWR Brand Management - WITH WEST BRANDS ALL.csv')
print(df_6.describe())  # get descriptive statistics

import pandas as pd
from scipy import stats

# List of columns in the DataFrame
columns = df_6.columns.tolist()

# Remove the column "Aus welchem Teil Deutschlands kommst du?" from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Calculate correlation and p-value of the column "Aus welchem Teil Deutschlands kommst du?" with all other columns
for col in columns:
    # Exclude NA/null values
    df_clean = df_6[['Aus welchem Teil Deutschlands kommst du?', col]].dropna()

    # Check if there are at least 2 non-null values
    if len(df_clean) >= 2:
        correlation, p_value = stats.pearsonr(df_clean['Aus welchem Teil Deutschlands kommst du?'], df_clean[col])
        print(f'Correlation of region with {col}: {correlation}, p-value: {p_value}')
    else:
        print(f'Not enough data to compute correlation between region and {col}')

from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy import stats
import numpy as np

columns = df_6.columns.tolist()

# Remove 'Aus welchem Teil Deutschlands kommst du?' from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Perform ANOVA and Tukey's HSD test for each column
for col in columns:
    group1 = df_6[df_6['Aus welchem Teil Deutschlands kommst du?'] == 1][col]
    group2 = df_6[df_6['Aus welchem Teil Deutschlands kommst du?'] == 2][col]

    # Concatenate the groups and create a list with labels
    values = pd.concat([group1, group2])
    labels = ['group1'] * len(group1) + ['group2'] * len(group2)

    # Perform the ANOVA
    f_val, p_val = stats.f_oneway(group1, group2)

    # Check for NaNs in ANOVA results
    if not (np.isnan(f_val) or np.isnan(p_val)):
        print(f'ANOVA results for {col}:\nF-value: {f_val}\np-value: {p_val}\n')

    # Perform Tukey's HSD test
    tukey = pairwise_tukeyhsd(endog=values,     # Data
                              groups=labels,   # Groups
                              alpha=0.05)      # Significance level

    # Print the result
    if not np.isnan(tukey.meandiffs).any():  # Check for NaNs in Tukey HSD results
        print(f"Tukey's HSD test results for {col}:\n{tukey}\n")

"""***US ALL BRANDS ISNIDE OF VARIATIONS WHERE US WAS***"""

df_7 = pd.read_csv('HWR Brand Management - US+US.csv')
print(df_7.describe())  # get descriptive statistics

import pandas as pd
from scipy import stats

# List of columns in the DataFrame
columns = df_7.columns.tolist()

# Remove the column "Aus welchem Teil Deutschlands kommst du?" from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Calculate correlation and p-value of the column "Aus welchem Teil Deutschlands kommst du?" with all other columns
for col in columns:
    # Exclude NA/null values
    df_clean = df_7[['Aus welchem Teil Deutschlands kommst du?', col]].dropna()

    # Check if there are at least 2 non-null values
    if len(df_clean) >= 2:
        correlation, p_value = stats.pearsonr(df_clean['Aus welchem Teil Deutschlands kommst du?'], df_clean[col])
        print(f'Correlation of region with {col}: {correlation}, p-value: {p_value}')
    else:
        print(f'Not enough data to compute correlation between region and {col}')

from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy import stats
import numpy as np

columns = df_7.columns.tolist()

# Remove 'Aus welchem Teil Deutschlands kommst du?' from the list
columns.remove('Aus welchem Teil Deutschlands kommst du?')

# Perform ANOVA and Tukey's HSD test for each column
for col in columns:
    group1 = df_7[df_7['Aus welchem Teil Deutschlands kommst du?'] == 1][col]
    group2 = df_7[df_7['Aus welchem Teil Deutschlands kommst du?'] == 2][col]

    # Concatenate the groups and create a list with labels
    values = pd.concat([group1, group2])
    labels = ['group1'] * len(group1) + ['group2'] * len(group2)

    # Perform the ANOVA
    f_val, p_val = stats.f_oneway(group1, group2)

    # Check for NaNs in ANOVA results
    if not (np.isnan(f_val) or np.isnan(p_val)):
        print(f'ANOVA results for {col}:\nF-value: {f_val}\np-value: {p_val}\n')

    # Perform Tukey's HSD test
    tukey = pairwise_tukeyhsd(endog=values,     # Data
                              groups=labels,   # Groups
                              alpha=0.05)      # Significance level

    # Print the result
    if not np.isnan(tukey.meandiffs).any():  # Check for NaNs in Tukey HSD results
        print(f"Tukey's HSD test results for {col}:\n{tukey}\n")



"""***CHARACTERESTICS AND QUESTIONS***"""

df_8 = pd.read_csv('HWR Brand Management - Characterestics.csv')
print(df_8.describe())  # get descriptive statistics

"""Individualism"""

import pandas as pd
import scipy.stats

# List of columns in the DataFrame
columns = df_8.columns.tolist()

# Remove the column "Individualism" from the list
columns.remove('Individualism')

# Calculate correlation of the column "Individualism" with all other columns
for col in columns:
    df_clean = df_8[['Individualism', col]].dropna()
    if len(df_clean) > 1:  # Check if cleaned data has at least 2 points
        correlation, p_value = scipy.stats.pearsonr(df_clean['Individualism'], df_clean[col])
        print(f'Correlation of Individualism with {col}: {correlation}, p-value: {p_value}')
    else:
        print(f"Not enough data to compute correlation for column '{col}'.")

"""Collectivism"""

import pandas as pd
import scipy.stats

# List of columns in the DataFrame
columns = df_8.columns.tolist()

# Remove the column "Individualism" from the list
columns.remove('Collectivism')

# Calculate correlation of the column "Individualism" with all other columns
for col in columns:
    df_clean = df_8[['Collectivism', col]].dropna()
    if len(df_clean) > 1:  # Check if cleaned data has at least 2 points
        correlation, p_value = scipy.stats.pearsonr(df_clean['Collectivism'], df_clean[col])
        print(f'Correlation of Collectivism with {col}: {correlation}, p-value: {p_value}')
    else:
        print(f"Not enough data to compute correlation for column '{col}'.")

"""Uncertainty Avoidance  

"""

import pandas as pd
import scipy.stats

# List of columns in the DataFrame
columns = df_8.columns.tolist()

# Remove the column "Individualism" from the list
columns.remove('Uncertainty Avoidance')

# Calculate correlation of the column "Individualism" with all other columns
for col in columns:
    df_clean = df_8[['Uncertainty Avoidance', col]].dropna()
    if len(df_clean) > 1:  # Check if cleaned data has at least 2 points
        correlation, p_value = scipy.stats.pearsonr(df_clean['Uncertainty Avoidance'], df_clean[col])
        print(f'Correlation of Collectivism with {col}: {correlation}, p-value: {p_value}')
    else:
        print(f"Not enough data to compute correlation for column '{col}'.")

